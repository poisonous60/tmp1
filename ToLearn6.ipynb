{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d90ef9c",
   "metadata": {},
   "source": [
    "---\n",
    "        \n",
    "연구주제: h.p. lovecraft 단편 소설 sna 분석\n",
    "   \n",
    "코드 구성:\n",
    "단편 하나를 문장단위로 끊어서 공출현 행렬을 만드는 함수 text2csv_of_co()\n",
    "\n",
    "단편 여러개에 대해 코사인 유사도 분석을 하는 함수 texts2csv_of_cossim()\n",
    "\n",
    "그리고 단편을 읽어와 두 함수를 적용하는 부분으로 나뉩니다.\n",
    "\n",
    "\n",
    "데이터셋 링크: <https://data.world/mattgawarecki/hp-lovecraft/workspace/project-summary?agentid=mattgawarecki&datasetid=hp-lovecraft>\n",
    "\n",
    "코사인 유사도 분석은 <https://wikidocs.net/book/2155>에 있는 코드를 썼습니다.\n",
    "\n",
    "---\n",
    "\n",
    "datetime: 2021/8/9 17:18:56 \n",
    "수정1: text2csv_of_co함수 주석 변경\n",
    "      stopwords 사용자 추가\n",
    "      text2csv_of_co함수에서 표제어 추출 TextBlob 안되는 거 확인하고 NLTK로 바꿈    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5896fbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install textblob\n",
    "# !pip install sklearn\n",
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda648bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jinhajin/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "import nltk\n",
    "import nltk.tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm.notebook import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d1536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2csv_of_co(lines: str, name: str):\n",
    "    lines_textblob = TextBlob(lines).lower()\n",
    "    \n",
    "    #문장 나누기\n",
    "    lines_made_sentence_with_textblob = [x.words for x in lines_textblob.sentences]\n",
    "    lines_sentence = [' '.join(list(x)) for x in lines_made_sentence_with_textblob]\n",
    "    # lines_sentence\n",
    "    \n",
    "    #토큰화\n",
    "    list_of_sentences_words = []\n",
    "    for line in lines_sentence:\n",
    "        text_tokens = word_tokenize(line)\n",
    "        a_sentence_words = [word for word in text_tokens if not word in stopwords.words()]\n",
    "        list_of_sentences_words.append(a_sentence_words)\n",
    "    # list_of_sentences_words\n",
    "    \n",
    "    \n",
    "    #표제어 추출\n",
    "    temp_list = []\n",
    "    for a_sentence_words in list_of_sentences_words:\n",
    "        temp = [WordNetLemmatizer().lemmatize(x, pos='v') for x in a_sentence_words]\n",
    "        temp2 = [''.join(x) for x in temp] #WordList타입을 str로 바꾸기 위한 코드\n",
    "        temp_list.append(temp2)\n",
    "    words_list = temp_list    #words_list가 최종 정리된 문장 리스트. 2차원 배열이고 lines_textblob.sentences()에 맞춰서 문장이 나눠져있음\n",
    "    \n",
    "    \n",
    "    #핵심어 정하기 위한 단어 빈도 세기\n",
    "    tokens = []\n",
    "    for a_sentence_words in list_of_sentences_words:\n",
    "        tokens.extend(a_sentence_words)\n",
    "    counted_tokens = Counter(tokens)\n",
    "    most_list = counted_tokens.most_common()\n",
    "    \n",
    "    \n",
    "    #본문에 3번이상 나온 단어를 핵심어로 정함\n",
    "    keywords_list = [string for string, fre in most_list if fre > 2] #(전처리한)문서 내 3번 이상 등장하는 단어만 핵심어.\n",
    "                                                                 # 이 리스트를 써서 csv를 만들거임\n",
    "    \n",
    "    #공출현 행렬 변수\n",
    "    df = pd.DataFrame(0, index=keywords_list, columns=keywords_list) #키워드 대칭행렬 만들거임\n",
    "\n",
    "    \n",
    "#     for idx, word in enumerate(keywords_list): #대각선에는 문서 내 핵심어의 빈도\n",
    "#         df[word][word] = most_list[idx][1]\n",
    "\n",
    "        \n",
    "    #문장단위 공출현 빈도 구하기\n",
    "    for word in keywords_list:\n",
    "        for line in lines_textblob.sentences:\n",
    "            count = 0\n",
    "            if line.find(word) != -1: #만약 핵심어 중 하나가 line 내에 있는 경우\n",
    "                for word2 in keywords_list: #그 외 핵심어도 위 line 내에 있는지 확인하고 count를 올림\n",
    "                    if word == word2:\n",
    "                        pass\n",
    "                    elif word != word2:\n",
    "                        if line.find(word2) != -1:\n",
    "                            df[word][word2] += 1\n",
    "                        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3867e55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def texts2csv_of_cossim(list_lines: list, list_name: list): #각 소설이 하나씩 들어가있는 1차원 배열\n",
    "    pd_list = pd.DataFrame(list_lines)\n",
    "    \n",
    "    #토큰화\n",
    "    pd_list[0] = pd_list.apply(lambda row: nltk.word_tokenize(row[0]), axis=1)\n",
    "    \n",
    "    #표제어 추출\n",
    "    pd_list[0] = pd_list[0].apply(lambda x: [WordNetLemmatizer().lemmatize(word, pos='v') for word in x])\n",
    "#     print(pd_list.head(5))\n",
    "    \n",
    "    #길이 2 이하 단어 제거\n",
    "    tokenized_doc = pd_list[0].apply(lambda x: [word for word in x if len(word) > 2])\n",
    "#     print(tokenized_doc[:5])\n",
    "    \n",
    "    # 역토큰화 (토큰화 작업을 되돌림)\n",
    "    detokenized_doc = []\n",
    "    for i in range(len(pd_list)):\n",
    "        t = ' '.join(tokenized_doc[i])\n",
    "        detokenized_doc.append(t)\n",
    "    \n",
    "    # 다시 pd_list[0]에 재저장\n",
    "    pd_list[0] = detokenized_doc \n",
    "    \n",
    "    #sklearn 라이브러리 함수로 TF-IDF 행렬 만들기\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', \n",
    "    max_features= 1000) # 상위 1,000개의 단어를 보존 \n",
    "    X = vectorizer.fit_transform(pd_list[0])\n",
    "    \n",
    "    #TF-IDF 행렬 이용해서 코사인 유사도 계산\n",
    "    cosine_sim = linear_kernel(X, X)\n",
    "    \n",
    "    #csv파일 만들기 위해 pandas 라이브러리의 DataFrame타입 이용\n",
    "    cosine_df = pd.DataFrame(cosine_sim, index=list_name, columns=list_name)\n",
    "\n",
    "    #csv로 내보내기\n",
    "#     cosine_df.to_csv('cosine_df.csv')\n",
    "\n",
    "    return cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2579eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Novel_name = \"\"\"he.txt\n",
    "unnamable.txt\n",
    "vault.txt\n",
    "what_moon_brings.txt\n",
    "whisperer.txt\n",
    "white_ship.txt\n",
    "high_house_mist.txt\n",
    "hound.txt\n",
    "hypnos.txt\n",
    "innsmouth.txt\n",
    "iranon.txt\n",
    "juan_romero.txt\n",
    "kadath.txt\n",
    "lurking_fear.txt\n",
    "martins_beach.txt\n",
    "medusas_coil.txt\n",
    "memory.txt\n",
    "moon_bog.txt\n",
    "mountains_of_madness.txt\n",
    "nameless.txt\n",
    "nyarlathotep.txt\n",
    "old_folk.txt\n",
    "other_gods.txt\n",
    "outsider.txt\n",
    "pickman.txt\n",
    "picture_house.txt\n",
    "poetry_and_the_gods.txt\n",
    "polaris.txt\n",
    "randolph_carter.txt\n",
    "rats_walls.txt\n",
    "reanimator.txt\n",
    "redhook.txt\n",
    "sarnath.txt\n",
    "shadow_out_of_time.txt\n",
    "shunned_house.txt\n",
    "silver_key.txt\n",
    "street.txt\n",
    "temple.txt\n",
    "terrible_old_man.txt\n",
    "tomb.txt\n",
    "tree.txt\n",
    "ulthar.txt\n",
    "under_the_pyramids.txt\n",
    "alchemist.txt\n",
    "arthur_jermyn.txt\n",
    "azathoth.txt\n",
    "beast.txt\n",
    "beyond_wall_of_sleep.txt\n",
    "book.txt\n",
    "celephais.txt\n",
    "charles_dexter_ward.txt\n",
    "clergyman.txt\n",
    "colour_out_of_space.txt\n",
    "cool_air.txt\n",
    "crawling_chaos.txt\n",
    "cthulhu.txt\n",
    "dagon.txt\n",
    "descendent.txt\n",
    "doorstep.txt\n",
    "dreams_in_the_witch.txt\n",
    "dunwich.txt\n",
    "erich_zann.txt\n",
    "ex_oblivione.txt\n",
    "festival.txt\n",
    "from_beyond.txt\n",
    "gates_of_silver_key.txt\n",
    "haunter.txt\"\"\".split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8caaffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Novel_list = []\n",
    "for name in Novel_name:\n",
    "    tag = \"../source/\" + name\n",
    "    f = open(tag, \"r\", encoding=\"utf-8\")\n",
    "    Novel_list.append(f.read().lower()) #전체 읽기\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8434b49d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Novel_cossim = texts2csv_of_cossim(Novel_list, Novel_name)\n",
    "Novel_cossim.to_csv('cosine_df_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a22672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c369c7a00d24cd3b91e03a941544599",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/67 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx in tqdm(range(len(Novel_list))):\n",
    "    co_csv = text2csv_of_co(Novel_list[idx], Novel_name[idx])\n",
    "    co_csv.to_csv('../csv2/' + Novel_name[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
